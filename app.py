"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ

ä½¿ç”¨æ–¹æ³•ï¼š
1. ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒPDFã€DOCXã€TXTæ ¼å¼ï¼‰
2. è¾“å…¥é—®é¢˜
3. è·å–AIå›ç­”
"""

import streamlit as st
import sys
import os
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import PyPDF2
import docx
import faiss
import tiktoken
import pickle
import numpy as np
from collections import Counter
import pandas as pd
import jieba
from dotenv import load_dotenv
import requests
import re
import time
import json

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def get_config(key: str, default: str = None) -> str:
    """
    è·å–é…ç½®å€¼ï¼Œä¼˜å…ˆä»Streamlit secretsè·å–ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»ç¯å¢ƒå˜é‡è·å–
    """
    try:
        return st.secrets[key]
    except KeyError:
        return os.getenv(key, default)

# æ›¿æ¢ç¯å¢ƒå˜é‡æ£€æŸ¥éƒ¨åˆ†
required_configs = {
    'OPENAI_API_KEY': 'ç”¨äºChatGPTçš„APIå¯†é’¥',
    'OPENAI_API_BASE': 'ChatGPTçš„APIåŸºç¡€URL',
    'DEEPSEEK_API_KEY': 'ç”¨äºDeepseekçš„APIå¯†é’¥',
    'DEEPSEEK_API_BASE': 'Deepseekçš„APIåŸºç¡€URL'
}

missing_configs = []
for config, description in required_configs.items():
    if not get_config(config):
        missing_configs.append(f"{config} ({description})")

if missing_configs:
    st.error("""
    ### ç¼ºå°‘å¿…è¦çš„é…ç½®
    è¯·åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€é…ç½®è¿™äº›å€¼ï¼š
    1. Streamlit Cloudéƒ¨ç½²ï¼šåœ¨é¡¹ç›®è®¾ç½®ä¸­æ·»åŠ  secrets.toml
    2. æœ¬åœ°å¼€å‘ï¼šåœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶
    
    ç¼ºå°‘çš„é…ç½®ï¼š
    """ + "\n".join(missing_configs))
    st.stop()

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - by Huaiyuan Tan",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# æ·»åŠ å¼€å‘è€…ä¿¡æ¯
st.markdown("<h6 style='text-align: right; color: gray;'>å¼€å‘è€…: Huaiyuan Tan</h6>", unsafe_allow_html=True)

# è‡ªå®šä¹‰æ ·å¼ï¼Œä½†ä¿ç•™Streamlité»˜è®¤UIå…ƒç´ 
custom_style = """
    <style>
    .reportview-container .main .block-container{
        padding-top: 1rem;
        padding-right: 1rem;
        padding-left: 1rem;
        padding-bottom: 1rem;
    }
    .stColumn {
        padding: 5px;
    }
    </style>
"""
st.markdown(custom_style, unsafe_allow_html=True)

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
chatgpt_client = OpenAI(
    api_key=get_config("OPENAI_API_KEY"),
    base_url=get_config("OPENAI_API_BASE"),
)

# å¯¹Deepseekçš„URLè¿›è¡Œå¤„ç†ï¼Œç¡®ä¿ä¸åŒ…å«chat/completions
deepseek_base_url = get_config("DEEPSEEK_API_BASE")
if deepseek_base_url and deepseek_base_url.endswith("/chat/completions"):
    deepseek_base_url = deepseek_base_url.replace("/chat/completions", "")

# åˆå§‹åŒ–Deepseekå®¢æˆ·ç«¯
deepseek_client = OpenAI(
    api_key=get_config("DEEPSEEK_API_KEY"),
    base_url=deepseek_base_url,
    default_headers={
        "Content-Type": "application/json",
        "Authorization": f"Bearer {get_config('DEEPSEEK_API_KEY')}"
    }
)

def test_api_connection(client, model_name):
    """æµ‹è¯•APIè¿æ¥çŠ¶æ€"""
    try:
        st.write(f"æ­£åœ¨æµ‹è¯• {model_name} APIè¿æ¥...")
        
        if model_name == "Deepseek":
            # ä½¿ç”¨é…ç½®å€¼è€Œä¸æ˜¯ç¡¬ç¼–ç 
            API_URL = f"{get_config('DEEPSEEK_API_BASE')}/chat/completions"
            API_KEY = get_config("DEEPSEEK_API_KEY")
            MODEL = "deepseek-ai/DeepSeek-R1"
            
            st.write(f"ä½¿ç”¨çš„APIåŸºç¡€URL: {API_URL}")
            st.write(f"ä½¿ç”¨çš„æ¨¡å‹: {MODEL}")
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {API_KEY}"
            }
            data = {
                "model": MODEL,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 10
            }
            response = requests.post(
                API_URL,
                headers=headers,
                json=data
            )
            if response.status_code != 200:
                raise Exception(f"APIè¿”å›é”™è¯¯: {response.text}")
            response_data = response.json()
            model_info = f"{model_name} ({response_data['model']})" if 'model' in response_data else model_name
            return True, "è¿æ¥æ­£å¸¸", model_info
        else:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            model_info = f"{model_name} ({response.model})" if hasattr(response, 'model') else model_name
            return True, "è¿æ¥æ­£å¸¸", model_info
    except Exception as e:
        st.error(f"{model_name} è¿æ¥æµ‹è¯•é”™è¯¯: {str(e)}")
        if model_name == "Deepseek":
            st.error(f"Deepseeké…ç½®ä¿¡æ¯ï¼š\nURL: {API_URL}\næ¨¡å‹: {MODEL}")
        return False, f"è¿æ¥é”™è¯¯: {str(e)}", model_name

# æµ‹è¯•APIè¿æ¥çŠ¶æ€
if 'api_status' not in st.session_state:
    st.session_state.api_status = {}

# åœ¨é¡µé¢é¡¶éƒ¨æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
st.markdown("## ğŸ¤– æ¨¡å‹çŠ¶æ€")

# åˆ›å»ºä¸¤åˆ—å¸ƒå±€æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
status_col1, status_col2 = st.columns(2)

# æµ‹è¯•å¹¶æ˜¾ç¤ºChatGPTçŠ¶æ€
with status_col1:
    st.markdown("### ChatGPT")
    chatgpt_ok, chatgpt_msg, chatgpt_model = test_api_connection(chatgpt_client, "ChatGPT")
    st.session_state.api_status['chatgpt'] = {
        'ok': chatgpt_ok,
        'message': chatgpt_msg,
        'model': chatgpt_model
    }
    if chatgpt_ok:
        st.success(f"âœ… {chatgpt_model}\n\nçŠ¶æ€ï¼š{chatgpt_msg}")
    else:
        st.error(f"âŒ {chatgpt_model}\n\nçŠ¶æ€ï¼š{chatgpt_msg}")

# æµ‹è¯•å¹¶æ˜¾ç¤ºDeepseekçŠ¶æ€
with status_col2:
    st.markdown("### Deepseek")
    deepseek_ok, deepseek_msg, deepseek_model = test_api_connection(deepseek_client, "Deepseek")
    st.session_state.api_status['deepseek'] = {
        'ok': deepseek_ok,
        'message': deepseek_msg,
        'model': deepseek_model
    }
    if deepseek_ok:
        st.success(f"âœ… {deepseek_model}\n\nçŠ¶æ€ï¼š{deepseek_msg}")
    else:
        st.error(f"âŒ {deepseek_model}\n\nçŠ¶æ€ï¼š{deepseek_msg}")

st.markdown("---")

@st.cache_resource
def load_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

model = load_model()

# è®¡ç®—tokenæ•°é‡
def num_tokens_from_string(string: str) -> int:
    encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    return len(encoding.encode(string))

# æ–‡æ¡£å‘é‡åŒ–æ¨¡å—
def vectorize_document(file, max_tokens):
    text = ""
    if file.type == "application/pdf":
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = docx.Document(file)
        for para in doc.paragraphs:
            text += para.text + "\n"
    else:
        text = file.getvalue().decode("utf-8")
    
    chunks = []
    current_chunk = ""
    for sentence in text.split('.'):
        if num_tokens_from_string(current_chunk + sentence) > max_tokens:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
        else:
            current_chunk += sentence + '.'
    if current_chunk:
        chunks.append(current_chunk)
    
    vectors = model.encode(chunks)
    index = faiss.IndexFlatL2(384)  # 384æ˜¯å‘é‡ç»´åº¦,æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
    index.add(vectors)
    return chunks, index

# æå–å…³é”®è¯
def extract_keywords(text, top_k=5):
    words = jieba.cut(text)
    word_count = Counter(words)
    # è¿‡æ»¤æ‰åœç”¨è¯å’Œå•ä¸ªå­—ç¬¦
    keywords = [word for word, count in word_count.most_common(top_k*2) if len(word) > 1]
    return keywords[:top_k]

# åŸºäºå…³é”®è¯æœç´¢æ–‡æ¡£
def search_documents(keywords, file_indices):
    relevant_docs = []
    for file_name, (chunks, _) in file_indices.items():
        doc_content = ' '.join(chunks)
        if any(keyword in doc_content for keyword in keywords):
            relevant_docs.append(file_name)
    return relevant_docs

# çŸ¥è¯†é—®ç­”æ¨¡å—
def rag_qa(query, file_indices, relevant_docs=None):
    try:
        # ä½¿ç”¨è¿›åº¦æ¡æ›¿ä»£æ™®é€šæ–‡æœ¬æç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # 1. å…³é”®è¯æå–å’Œæ–‡æ¡£æœç´¢ (10%)
        status_text.text("æ­£åœ¨åˆ†æé—®é¢˜å…³é”®è¯...")
        keywords = extract_keywords(query)
        if relevant_docs is None:
            relevant_docs = search_documents(keywords, file_indices)
        progress_bar.progress(10)
        
        if not relevant_docs:
            status_text.error("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚")
            return {
                'chatgpt': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚",
                'deepseek': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚"
            }, [], ""

        # 2. å‘é‡æ£€ç´¢å‡†å¤‡ (20%)
        status_text.text("æ­£åœ¨å‡†å¤‡ç›¸å…³æ–‡æ¡£å†…å®¹...")
        all_chunks = []
        chunk_to_file = {}
        combined_index = faiss.IndexFlatL2(384)
        
        offset = 0
        for file_name in relevant_docs:
            if file_name in file_indices:
                chunks, index = file_indices[file_name]
                all_chunks.extend(chunks)
                for i in range(len(chunks)):
                    chunk_to_file[offset + i] = file_name
                if index.ntotal > 0:
                    vectors = index.reconstruct_n(0, index.ntotal)
                    combined_index.add(vectors.astype(np.float32))
                offset += len(chunks)
        progress_bar.progress(20)

        if not all_chunks:
            status_text.error("æ— æ³•ä»æ–‡æ¡£ä¸­æå–å†…å®¹ï¼Œè¯·ç¡®ä¿æ–‡æ¡£å·²æ­£ç¡®ä¸Šä¼ ã€‚")
            return {
                'chatgpt': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ æ–‡æ¡£ã€‚",
                'deepseek': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ æ–‡æ¡£ã€‚"
            }, [], ""

        # 3. æ‰§è¡Œå‘é‡æ£€ç´¢ (30%)
        status_text.text("æ­£åœ¨æ£€ç´¢æœ€ç›¸å…³çš„å†…å®¹ç‰‡æ®µ...")
        query_vector = model.encode([query])
        D, I = combined_index.search(query_vector.astype(np.float32), k=3)
        context = []
        context_with_sources = []
        for i in I[0]:
            if 0 <= i < len(all_chunks):
                chunk = all_chunks[i]
                context.append(chunk)
                file_name = chunk_to_file.get(i, "æœªçŸ¥æ–‡ä»¶")
                context_with_sources.append((file_name, chunk))
        progress_bar.progress(30)

        # 4. å‡†å¤‡ä¸Šä¸‹æ–‡ (40%)
        status_text.text("æ­£åœ¨æ•´ç†ä¸Šä¸‹æ–‡ä¿¡æ¯...")
        context_text = "\n".join(context)
        max_context_tokens = 3000
        original_length = len(context_text)
        while num_tokens_from_string(context_text) > max_context_tokens:
            context_text = context_text[:int(len(context_text)*0.9)]
        progress_bar.progress(40)
        
        if not context_text:
            status_text.error("æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„ä¸Šä¸‹æ–‡å†…å®¹ã€‚")
            return {
                'chatgpt': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                'deepseek': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            }, [], ""

        # 5. åˆ›å»ºUIå¸ƒå±€ (45%)
        left_col, right_col = st.columns(2)
        with left_col:
            st.markdown("### ChatGPTå›ç­”")
            chatgpt_placeholder = st.empty()
            chatgpt_placeholder.markdown("""
            <div style='background-color: #f0f2f6; padding: 15px; border-radius: 5px;'>
                <div class="loading">æ­£åœ¨ç­‰å¾…ChatGPTå›ç­”...</div>
            </div>
            """, unsafe_allow_html=True)
        
        with right_col:
            st.markdown("### Deepseekå›ç­”")
            # åˆ›å»ºæ¨ç†è¿‡ç¨‹çš„expander
            thinking_expander = st.expander("ğŸ¤” æŸ¥çœ‹Deepseekå®æ—¶æ¨ç†è¿‡ç¨‹", expanded=True)
            with thinking_expander:
                st.markdown("""
                <div style='background-color: #f0f2f6; padding: 10px; border-radius: 5px; margin-bottom: 10px;'>
                    <p style='color: #666; margin-bottom: 10px;'>ç­‰å¾…å¼€å§‹æ¨ç†...</p>
                </div>
                """, unsafe_allow_html=True)
                thinking_placeholder = st.empty()
            
            deepseek_placeholder = st.empty()
            deepseek_placeholder.markdown("""
            <div style='background-color: #e6f3ff; padding: 15px; border-radius: 5px;'>
                <div class="loading">ç­‰å¾…å¼€å§‹å¤„ç†...</div>
            </div>
            """, unsafe_allow_html=True)

        responses = {'chatgpt': "", 'deepseek': ""}
        excerpts = {'chatgpt': "", 'deepseek': ""}
        progress_bar.progress(45)

        # 6. å¤„ç†ChatGPTå›ç­” (45-70%)
        status_text.text("æ­£åœ¨è·å–ChatGPTå›ç­”...")
        if st.session_state.api_status['chatgpt']['ok']:
            try:
                chatgpt_response = chatgpt_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œæ— è®ºé—®é¢˜æ˜¯ä»€ä¹ˆè¯­è¨€ã€‚åœ¨å›ç­”ä¹‹åï¼Œè¯·åŠ¡å¿…æä¾›ä¸€æ®µæœ€ç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"},
                        {"role": "user", "content": f"ä¸Šä¸‹æ–‡: {context_text}\n\né—®é¢˜: {query}\n\nè¯·æä¾›ä½ çš„å›ç­”ç„¶ååœ¨å›ç­”åé¢é™„ä¸Šç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"}
                    ]
                )
                chatgpt_answer = chatgpt_response.choices[0].message.content
                
                if "ç›¸å…³åŸæ–‡ï¼š" in chatgpt_answer:
                    chatgpt_parts = chatgpt_answer.split("ç›¸å…³åŸæ–‡ï¼š", 1)
                    responses['chatgpt'] = chatgpt_parts[0].strip()
                    excerpts['chatgpt'] = chatgpt_parts[1].strip()
                else:
                    responses['chatgpt'] = chatgpt_answer.strip()
                
                # ç«‹å³æ›´æ–°ChatGPTå›ç­”
                chatgpt_placeholder.markdown(f"""
                <div style='background-color: #f0f2f6; padding: 15px; border-radius: 5px;'>
                    {responses['chatgpt']}
                </div>
                """, unsafe_allow_html=True)
            except Exception as e:
                st.error(f"ChatGPT APIè°ƒç”¨å‡ºé”™: {str(e)}")
                responses['chatgpt'] = "APIè°ƒç”¨å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•"
                chatgpt_placeholder.error("è·å–ChatGPTå›ç­”å¤±è´¥")
        else:
            responses['chatgpt'] = "ChatGPT API æœªè¿æ¥"
            chatgpt_placeholder.error("ChatGPT API æœªè¿æ¥")
        progress_bar.progress(70)

        # 7. å¤„ç†Deepseekå›ç­” (70-95%)
        status_text.text("æ­£åœ¨è·å–Deepseekå›ç­”...")
        deepseek_placeholder.markdown("""
        <div style='background-color: #e6f3ff; padding: 15px; border-radius: 5px;'>
            <div class="loading">æ­£åœ¨å¤„ç†Deepseekå›ç­”...</div>
        </div>
        """, unsafe_allow_html=True)
        
        if st.session_state.api_status['deepseek']['ok']:
            try:
                # æ›´æ–°æ¨ç†è¿‡ç¨‹çš„çŠ¶æ€æç¤º
                with thinking_expander:
                    st.markdown("""
                    <div style='background-color: #f0f2f6; padding: 10px; border-radius: 5px; margin-bottom: 10px;'>
                        <p style='color: #666; margin-bottom: 10px;'>æ­£åœ¨è¿›è¡Œæ¨ç†...</p>
                    </div>
                    """, unsafe_allow_html=True)
                
                API_URL = f"{get_config('DEEPSEEK_API_BASE')}/chat/completions"
                API_KEY = get_config("DEEPSEEK_API_KEY")
                MODEL = "deepseek-ai/DeepSeek-R1"
                
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {API_KEY}"
                }
                data = {
                    "model": MODEL,
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œæ— è®ºé—®é¢˜æ˜¯ä»€ä¹ˆè¯­è¨€ã€‚åœ¨å›ç­”ä¹‹åï¼Œè¯·åŠ¡å¿…æä¾›ä¸€æ®µæœ€ç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚åœ¨å›ç­”è¿‡ç¨‹ä¸­ï¼Œè¯·ä½¿ç”¨'/think/ä½ çš„æ¨ç†è¿‡ç¨‹/think/'çš„æ ¼å¼æ¥å±•ç¤ºä½ çš„æ¨ç†è¿‡ç¨‹ã€‚"},
                        {"role": "user", "content": f"ä¸Šä¸‹æ–‡: {context_text}\n\né—®é¢˜: {query}\n\nè¯·ä¸€æ­¥æ­¥æ€è€ƒå¹¶å›ç­”è¿™ä¸ªé—®é¢˜ã€‚åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ï¼Œç”¨'/think/ä½ çš„æ¨ç†è¿‡ç¨‹/think/'æ ¼å¼æ¥å±•ç¤ºä½ çš„æ¨ç†è¿‡ç¨‹ï¼Œæœ€åæä¾›å®Œæ•´ç­”æ¡ˆå’Œç›¸å…³åŸæ–‡ã€‚"}
                    ],
                    "max_tokens": 1000,
                    "stream": True
                }
                
                # ä½¿ç”¨streamæ¨¡å¼å‘é€è¯·æ±‚
                response = requests.post(
                    API_URL,
                    headers=headers,
                    json=data,
                    stream=True
                )
                
                if response.status_code != 200:
                    error_detail = response.json() if response.text else "æ— è¯¦ç»†é”™è¯¯ä¿¡æ¯"
                    raise Exception(f"APIè¿”å›é”™è¯¯: {error_detail}")
                
                # ç”¨äºå­˜å‚¨å®Œæ•´çš„å“åº”
                full_response = ""
                current_think = ""
                current_answer = ""
                think_count = 0
                
                # åˆ›å»ºä¸€ä¸ªç©ºçš„å®¹å™¨ç”¨äºæ˜¾ç¤ºå®æ—¶æ¨ç†è¿‡ç¨‹
                thinking_container = thinking_placeholder.container()
                
                # å¤„ç†æµå¼å“åº”
                for line in response.iter_lines():
                    if line:
                        # ç§»é™¤"data: "å‰ç¼€å¹¶è§£æJSON
                        json_str = line.decode('utf-8').replace('data: ', '')
                        if json_str.strip() == '[DONE]':
                            break
                        try:
                            chunk = json.loads(json_str)
                            if chunk.get('choices') and chunk['choices'][0].get('delta', {}).get('content'):
                                content = chunk['choices'][0]['delta']['content']
                                full_response += content
                                
                                # æ£€æŸ¥æ˜¯å¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­
                                if '/think/' in content:
                                    if current_think:
                                        # å¦‚æœå·²ç»æœ‰ä¸€ä¸ªæ¨ç†è¿‡ç¨‹åœ¨è¿›è¡Œï¼Œå…ˆä¿å­˜å®ƒ
                                        think_count += 1
                                        thinking_container.markdown(f"""
                                        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0; border: 1px solid #e9ecef;'>
                                            <div style='color: #495057; margin-bottom: 8px;'><strong>ğŸ”„ æ¨ç†æ­¥éª¤ {think_count}</strong></div>
                                            <div style='color: #212529;'>{current_think.strip()}</div>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    current_think = content.replace('/think/', '')
                                elif current_think is not None:
                                    if '/think/' in content:  # ç»“æŸå½“å‰æ¨ç†
                                        current_think = current_think.replace('/think/', '')
                                        think_count += 1
                                        thinking_container.markdown(f"""
                                        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0; border: 1px solid #e9ecef;'>
                                            <div style='color: #495057; margin-bottom: 8px;'><strong>ğŸ”„ æ¨ç†æ­¥éª¤ {think_count}</strong></div>
                                            <div style='color: #212529;'>{current_think.strip()}</div>
                                        </div>
                                        """, unsafe_allow_html=True)
                                        current_think = None
                                    else:
                                        current_think += content
                                        # å®æ—¶æ›´æ–°å½“å‰æ¨ç†æ­¥éª¤
                                        thinking_container.markdown(f"""
                                        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0; border: 1px solid #e9ecef;'>
                                            <div style='color: #495057; margin-bottom: 8px;'><strong>ğŸ”„ æ¨ç†æ­¥éª¤ {think_count + 1} (è¿›è¡Œä¸­...)</strong></div>
                                            <div style='color: #212529;'>{current_think.strip()}</div>
                                        </div>
                                        """, unsafe_allow_html=True)
                                else:
                                    current_answer += content
                                    # å®æ—¶æ›´æ–°ç­”æ¡ˆ
                                    if not content.startswith('/think/'):
                                        deepseek_placeholder.markdown(f"""
                                        <div style='background-color: #e6f3ff; padding: 15px; border-radius: 5px;'>
                                            {current_answer}
                                        </div>
                                        """, unsafe_allow_html=True)
                                
                        except json.JSONDecodeError:
                            continue
                
                # è§£ææœ€ç»ˆç­”æ¡ˆ
                deepseek_answer = full_response
                
                # ç§»é™¤æ‰€æœ‰æ¨ç†è¿‡ç¨‹ï¼Œå¾—åˆ°æœ€ç»ˆç­”æ¡ˆ
                final_answer = re.sub(r'/think/.*?/think/', '', deepseek_answer, flags=re.DOTALL)
                
                # å¤„ç†ç›¸å…³åŸæ–‡
                if "ç›¸å…³åŸæ–‡ï¼š" in final_answer:
                    answer_parts = final_answer.split("ç›¸å…³åŸæ–‡ï¼š", 1)
                    responses['deepseek'] = answer_parts[0].strip()
                    excerpts['deepseek'] = answer_parts[1].strip()
                else:
                    responses['deepseek'] = final_answer.strip()
                
                # æ›´æ–°Deepseekå›ç­”æ˜¾ç¤º
                deepseek_content = f"""
                <div style='background-color: #e6f3ff; padding: 15px; border-radius: 5px;'>
                    {responses['deepseek']}
                </div>
                """
                deepseek_placeholder.markdown(deepseek_content, unsafe_allow_html=True)
                
            except Exception as e:
                responses['deepseek'] = "APIè°ƒç”¨å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•"
                deepseek_placeholder.error("è·å–Deepseekå›ç­”å¤±è´¥")
        else:
            responses['deepseek'] = "Deepseek API æœªè¿æ¥"
            deepseek_placeholder.error("Deepseek API æœªè¿æ¥")
        progress_bar.progress(95)

        # 8. æ˜¾ç¤ºè¡¥å……ä¿¡æ¯ (95-100%)
        status_text.text("æ­£åœ¨æ•´ç†è¡¥å……ä¿¡æ¯...")
        
        # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
        if context_with_sources:
            st.markdown("### æ¥æºæ–‡æ¡£")
            for file_name, context in context_with_sources:
                with st.expander(f"ğŸ“„ {file_name}"):
                    st.write(context)
        
        # æ˜¾ç¤ºç›¸å…³åŸæ–‡
        excerpt = excerpts['chatgpt'] or excerpts['deepseek']
        if excerpt:
            st.markdown("### ç›¸å…³åŸæ–‡")
            st.markdown(f"""
            <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; border: 1px solid #dee2e6;'>
                {excerpt}
            </div>
            """, unsafe_allow_html=True)

        progress_bar.progress(100)
        status_text.text("å¤„ç†å®Œæˆï¼")
        
        # æ¸…ç†ä¸´æ—¶UIå…ƒç´ 
        time.sleep(0.5)  # ç»™ç”¨æˆ·ä¸€ä¸ªçŸ­æš‚çš„æ—¶é—´çœ‹åˆ°å®ŒæˆçŠ¶æ€
        progress_bar.empty()
        status_text.empty()

        return responses, context_with_sources, excerpt

    except Exception as e:
        if 'progress_bar' in locals():
            progress_bar.empty()
        if 'status_text' in locals():
            status_text.empty()
        st.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        st.error(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
        return {
            'chatgpt': "å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•",
            'deepseek': "å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•"
        }, [], ""

# ä¿å­˜ç´¢å¼•å’Œchunks
def save_index(file_name, chunks, index):
    if not os.path.exists('indices'):
        os.makedirs('indices')
    with open(f'indices/{file_name}.pkl', 'wb') as f:
        pickle.dump((chunks, index), f)
    # ä¿å­˜æ–‡ä»¶ååˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
    else:
        file_list = []
    if file_name not in file_list:
        file_list.append(file_name)
        with open(file_list_path, 'w') as f:
            f.write('\n'.join(file_list))

# åŠ è½½æ‰€æœ‰ä¿å­˜çš„ç´¢å¼•
def load_all_indices():
    file_indices = {}
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
        for file_name in file_list:
            file_path = f'indices/{file_name}.pkl'
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    chunks, index = pickle.load(f)
                file_indices[file_name] = (chunks, index)
    return file_indices

def delete_index(file_name):
    if os.path.exists(f'indices/{file_name}.pkl'):
        os.remove(f'indices/{file_name}.pkl')
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
        if file_name in file_list:
            file_list.remove(file_name)
            with open(file_list_path, 'w') as f:
                f.write('\n'.join(file_list))

def main():
    st.markdown("""
    <style>
    .reportview-container .main .block-container{
        padding-top: 1rem;
        padding-right: 1rem;
        padding-left: 1rem;
        padding-bottom: 1rem;
    }
    .stColumn {
        padding: 5px;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")

    # åˆå§‹åŒ– session state
    if "file_indices" not in st.session_state:
        st.session_state.file_indices = load_all_indices()

    # æ–‡æ¡£ä¸Šä¼ éƒ¨åˆ†
    st.subheader("æ–‡æ¡£ä¸Šä¼ ")
    
    max_tokens = 4096

    uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "docx", "txt"], accept_multiple_files=True)

    if uploaded_files:
        for uploaded_file in uploaded_files:
            with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£: {uploaded_file.name}..."):
                chunks, index = vectorize_document(uploaded_file, max_tokens)
                st.session_state.file_indices[uploaded_file.name] = (chunks, index)
                save_index(uploaded_file.name, chunks, index)
            st.success(f"æ–‡æ¡£ {uploaded_file.name} å‘é‡åŒ–å¹¶æ·»åŠ åˆ°ç´¢å¼•ä¸­ï¼")

    # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡ä»¶å¹¶æ·»åŠ åˆ é™¤æŒ‰é’®
    st.subheader("å·²å¤„ç†æ–‡æ¡£:")
    for file_name in list(st.session_state.file_indices.keys()):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.write(f"â€¢ {file_name}")
        with col2:
            if st.button("åˆ é™¤", key=f"delete_{file_name}"):
                del st.session_state.file_indices[file_name]
                delete_index(file_name)
                st.success(f"æ–‡æ¡£ {file_name} å·²åˆ é™¤ï¼")
                st.rerun()

    # æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
    st.subheader("å…³é”®è¯æœç´¢")
    search_keywords = st.text_input("è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰")
    if search_keywords:
        keywords = search_keywords.split()
        relevant_docs = search_documents(keywords, st.session_state.file_indices)
        if relevant_docs:
            st.write("ç›¸å…³æ–‡æ¡£ï¼š")
            for doc in relevant_docs:
                st.write(f"â€¢ {doc}")
            st.session_state.relevant_docs = relevant_docs
        else:
            st.write("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
            st.session_state.relevant_docs = None

    # é—®ç­”éƒ¨åˆ†
    st.subheader("é—®ç­”")
    query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
    
    if query:
        try:
            with st.spinner("æ­£åœ¨æŸ¥æ‰¾ç­”æ¡ˆ..."):
                # åªè°ƒç”¨rag_qaå‡½æ•°ï¼Œä¸å†é‡å¤æ˜¾ç¤ºç»“æœ
                rag_qa(
                    query, 
                    st.session_state.file_indices,
                    st.session_state.get('relevant_docs')
                )
        except Exception as e:
            st.error(f"å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            st.error(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
            st.info("è¯·æ£€æŸ¥APIé…ç½®æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¨åé‡è¯•")

if __name__ == "__main__":
    main() 